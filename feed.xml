<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://gohugo.io/" version="0.124.1">Hugo</generator><title>DiagnoseAI Development Blog</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/" rel="alternate" type="text/html" title="html"/><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/feed.xml" rel="self" type="application/atom+xml" title="atom"/><updated>2024-03-24T17:18:00+00:00</updated><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/</id><entry><title>Week 11 - Continuing Work Orders</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-11/" rel="alternate" type="text/html" hreflang="en"/><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-11/</id><published>2024-02-19T00:00:00+00:00</published><updated>2024-02-19T00:00:00+00:00</updated><content type="html">
&lt;p>This week (Monday was 19 November 2024) was spent continuing the Work we started last week on Work Orders. Further work was needed in developing the Work Orders in the Uploader and Web App pages.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="work-orders-in-uploader"
>
Work Orders in Uploader
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-11/#work-orders-in-uploader" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Work Orders in Uploader" href="#work-orders-in-uploader">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>During the previous week, the front-end UI for the uploader page was successfully created. However, for Work Orders to be fully implemented onto the Uploader page a few key tasks were stil required:&lt;/p>
&lt;ul>
&lt;li>API endpoints to create and delete Work Orders and to fetch the available Machines&lt;/li>
&lt;li>Global State management system to store selected Machine from upload to validation page&lt;/li>
&lt;/ul>
&lt;p>Here are a few code snippets from the aforementioned tasks:&lt;/p>
&lt;p>Code snippet of API endpoint to fetch machines:&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/fetch-machines-api.png" alt="Image of the Fetch Machines API code" />
&lt;/p>
&lt;/p>
&lt;p>Code snippet of Global Storage with redux:
&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/redux-store.png" alt="Image of the Global state management code snippet" />
&lt;/p>
&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="work-orders-in-webapp"
>
Work Orders in WebApp
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-11/#work-orders-in-webapp" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Work Orders in WebApp" href="#work-orders-in-webapp">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>Similar to the Uploader page, the front-end for the Work orders was created in the Web App last week. However, more work was needed to fetch and provide the Work Order details to the front end.&lt;/p>
&lt;ul>
&lt;li>Creation of a context to provide the chat with work order selection details&lt;/li>
&lt;li>Creation of an API endpoint to fetch the Work orders&lt;/li>
&lt;/ul>
&lt;p>Here are a few code snippets from the aforementioned tasks:&lt;/p>
&lt;p>Code snippet of the API endpoint to fetch Work Orders:
&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/work-order-api.png" alt="Image of the WorkOrders endpoint" />
&lt;/p>
&lt;/p>
&lt;p>Code snippet of the Work Order context:
&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/work-order-context.png" alt="Image of the Work Order context" />
&lt;/p>
&lt;/p></content></entry><entry><title>Week 10 - Deployment</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-10/" rel="alternate" type="text/html" hreflang="en"/><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-10/</id><published>2024-02-12T00:00:00+00:00</published><updated>2024-02-12T00:00:00+00:00</updated><content type="html">
&lt;p>This week, there were two main tasks for our team:&lt;/p>
&lt;ol>
&lt;li>&lt;a
class="gblog-markdown__link"
href="#work-orders"
>Work Orders&lt;/a>&lt;/li>
&lt;li>&lt;a
class="gblog-markdown__link"
href="#deployment"
>Deployment&lt;/a>&lt;/li>
&lt;/ol>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="work-orders"
>
Work Orders
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-10/#work-orders" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Work Orders" href="#work-orders">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>As defined by our client, a Work Order is a record that stores
information about a job involving a particular type of machine. In
particular, for our interests, it defines:&lt;/p>
&lt;ol>
&lt;li>The machine being worked on (i.e. Machine Name, Machine Model);&lt;/li>
&lt;li>The engineer on the job (i.e. User ID).&lt;/li>
&lt;/ol>
&lt;p>Our application relies on Work Orders to further contextualize the
conversation by only pulling in the documents that really matter to
the job at hand.&lt;/p>
&lt;p>To this end, we had four main sub-tasks:&lt;/p>
&lt;ol>
&lt;li>The Uploader interface needed a way to upload a document tied to a
specific machine;&lt;/li>
&lt;li>The Uploader interface, for the purposes of demonstration, should
be able to create / delete work orders&lt;/li>
&lt;li>CoreAPI needs to provide any connected interfaces access to all
Work Orders owned by the user&lt;/li>
&lt;li>The Web App interface needs to fetch all Work Orders&lt;/li>
&lt;/ol>
&lt;p>In our system, Work Orders are akin to individual conversations found
in other chatbot + LLM applications like ChatGPT. Each Work Order
defines one conversation.&lt;/p>
&lt;p>We then defined the schema, using the &lt;code>sqlalchemy&lt;/code> ORM package so that
we don&amp;rsquo;t have to deal with the database directly:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">WorkOrderModel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Base&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">__tablename__&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;work_orders&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">order_id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Mapped&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mapped_column&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">String&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">36&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">primary_key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">lambda&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uuid4&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">user_id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Mapped&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mapped_column&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">String&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">36&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="c1"># Assuming user_id is string from clerk&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">conversation_id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Mapped&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mapped_column&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">String&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">36&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">lambda&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uuid4&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">machine_id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Mapped&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mapped_column&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">String&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">36&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">ForeignKey&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;machines.machine_id&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">task_name&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Mapped&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mapped_column&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">255&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">task_desc&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Mapped&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mapped_column&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">created_at&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Mapped&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">DateTime&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mapped_column&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">DateTime&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">func&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">now&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="c1"># Automatically sets to current time&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">machine&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">relationship&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;MachineModel&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">back_populates&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;work_orders&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">MachineModel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Base&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">__tablename__&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;machines&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">machine_id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Mapped&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mapped_column&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">String&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">36&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">primary_key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">lambda&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uuid4&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">manufacturer&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Mapped&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mapped_column&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">255&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Mapped&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mapped_column&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">255&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">work_orders&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">relationship&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;WorkOrderModel&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">back_populates&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;machine&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here, each Work Order owns a Machine, while each Machine is associated
to many Work Orders.&lt;/p>
&lt;p>Here are some screenshots of the resultant handiwork:&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/2024-02-19_01.png" alt="Uploader Interface" />
&lt;/p>
&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/2024-02-19_02.png" alt="WebApp Interface" />
&lt;/p>
&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="deployment"
>
Deployment
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-10/#deployment" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Deployment" href="#deployment">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>For the past few weeks, we have been doing our best to migrate
endpoints and keys to the server-side so that we can publicly deploy
the frontends.&lt;/p>
&lt;p>The CoreAPI was already deployed to begin with, given it&amp;rsquo;s serverless
nature; however, the frontends had never seen the light of the day.&lt;/p>
&lt;p>The URLs are not made public for obvious reasons, but we are proud to
announce that we have successfully deployed the two applications to
the public.&lt;/p>
&lt;p>Right now, our deployment pipeline looks like this:&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/2024-02-19_03.png" alt="GitHub Actions" />
&lt;/p>
&lt;/p>
&lt;p>On push to &lt;code>main&lt;/code>, this GitHub Actions will trigger, which builds the
frontend and packages them. We found that the default GitHub Actions
workflow file was not sufficient for our use; hence, we modified it so
that it worked for our use case.&lt;/p>
&lt;p>In particular, we first had to modify our NextJS build outputs to
&lt;code>standalone&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-javascript" data-lang="javascript">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/** @type {import(&amp;#39;next&amp;#39;).NextConfig} */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kr">const&lt;/span> &lt;span class="nx">nextConfig&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">reactStrictMode&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">true&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">output&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s2">&amp;#34;standalone&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nx">module&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">exports&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">nextConfig&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then, we had to modify the job for GitHub actions to also package our
static resources:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">npm install and build&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">env&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">${{ secrets.NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY }}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">CLERK_SECRET_KEY&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">${{ secrets.CLERK_SECRET_KEY }}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">run&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">|&lt;/span>&lt;span class="sd">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> npm install
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> npm run build
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> mkdir -p .next/standalone/.next
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> cp -r .next/static .next/standalone/.next/&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Zip artifact for deployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">run&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">|&lt;/span>&lt;span class="sd">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> cd .next/standalone/
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> zip ../../release.zip ./* .next -r
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> cd -&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>At the same time, we also had to pass the
&lt;code>NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY&lt;/code> secret so that Clerk could be
properly built for production deployment.&lt;/p>
&lt;p>After which, we had to do a bunch of Azure changes, namely:&lt;/p>
&lt;ol>
&lt;li>In Core API, we had to adjust our &lt;code>AZP_LIST&lt;/code> for Clerk (the
accepted domains that Clerk works on) to include our new deployed
domains;&lt;/li>
&lt;li>Create new Web Apps on Azure to support both the uploader and the
webapp. We experimented with Static Web Apps with their &lt;a
class="gblog-markdown__link"
href="https://learn.microsoft.com/en-us/azure/static-web-apps/deploy-nextjs-hybrid"
>preview hybrid
Next.JS
support&lt;/a>,
but it did not properly support the authentication middleware used
by Clerk; hence, we decided to go for Web Apps instead. In terms of
cost, Static Web Apps would have been less expensive; hence, once
the feature comes out of preview, we will advise the client to move
to Static Web Apps&lt;/li>
&lt;li>Create firewall rules on our Azure SQL Database to allow connection
from any Azure resource, and also the IP address of the Uploader
Web App.&lt;/li>
&lt;/ol></content></entry><entry><title>Week 9 - Enhancements &amp; Deployment Preparation</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-09/" rel="alternate" type="text/html" hreflang="en"/><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-09/</id><published>2024-01-30T00:00:00+00:00</published><updated>2024-01-30T00:00:00+00:00</updated><content type="html">
&lt;p>The goal of Week 9 was to progress with the &amp;ldquo;Must Have&amp;rdquo; requirements. As more
tasks are being moved to the &amp;ldquo;Done&amp;rdquo; section of the Kanban Board, the team
started thinking about deployment. This is something to be done eventually, and
thinking ahead is beneficial in this case as many bugs can arise when it comes
to deployment.&lt;/p>
&lt;p>The components addressed this week are webapp and uploader. Both of these are
ready for deployment, which planned for next week.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="webapp-enchancements"
>
Webapp enchancements
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-09/#webapp-enchancements" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Webapp enchancements" href="#webapp-enchancements">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>As the functionality of the webapp extends and gives the user more options (such
as uploading images), it is important to provide an intuitive UI. We took
inspiration from other chatbot applications such as ChatGPT and Gemini and made
polishes described below:&lt;/p>
&lt;ul>
&lt;li>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h2 id="ui--websocket-polishes"
>
UI &amp;amp; WebSocket polishes
&lt;/h2>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-09/#ui--websocket-polishes" class="gblog-post__anchor clip flex align-center" aria-label="Anchor UI &amp;amp; WebSocket polishes" href="#ui--websocket-polishes">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>The UI changes focus on providing the user with more information about what
state the app is currently in. To achieve this, webapp now displays a
placeholder message (&amp;ldquo;Processing image&amp;hellip;&amp;rdquo;) as an image is being processed,
until genuine text is returned. In addition, if an image is currently selected
its path will be shown in the input box. These changes can be seen in the
images below.&lt;/p>
&lt;p>Image path:&lt;/p>
&lt;img src="img_path.png" alt="Image of new image send UI" width=860>
&lt;p>Temporary bot message:&lt;/p>
&lt;img src="processing_img.png" alt="Image of photo processing" width=500>
&lt;p>As part of enhancing safety, as well as UX, typing and sending
messages is disallowed under certain conditions - WebSocket disconnections,
an image from the user curently being processed, and if an image is
currently selected for upload. Crucially, whenever the WebSocket disconnects
the webapp will try to reconnect it automatically. This change ensures that
the user will not need to manually refresh the page in case of disconnections.&lt;/p>
&lt;/li>
&lt;li>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h2 id="chat-history"
>
Chat history
&lt;/h2>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-09/#chat-history" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Chat history" href="#chat-history">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>The idea of this enhancement is to allow the user to continue a conversation
that she has previously started. Before history was implemented, only messages
sent in the current session would be saved locally; if the page was refreshed,
then the conversation would be wiped. &lt;code>ChatContext&lt;/code> now has a
&lt;code>fetchHistory(conversationId)&lt;/code> function to fetch the entire context based on
the conversation belonging to the current user. This will allow users to see
their previous messages sent in a different session or from a different device&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="uploader-enhancements"
>
Uploader enhancements
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-09/#uploader-enhancements" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Uploader enhancements" href="#uploader-enhancements">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>The uploader changes for this week are less comprehensive, but essential nonetheless.&lt;/p>
&lt;ul>
&lt;li>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h2 id="deployment-preparations"
>
Deployment Preparations
&lt;/h2>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-09/#deployment-preparations" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Deployment Preparations" href="#deployment-preparations">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>Similar to the API key changes from &lt;a
class="gblog-markdown__link"
href="/diagnoseai-dev-blog/posts/week-08/#api-key-security"
>Week 8&lt;/a>,
the uploader is now internet ready, it does not expose any keys. Therefore,
both frontends can now be deployed.&lt;/p>
&lt;/li>
&lt;li>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h2 id="uploader-calls-verification"
>
Uploader calls verification
&lt;/h2>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-09/#uploader-calls-verification" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Uploader calls verification" href="#uploader-calls-verification">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>An important change that was started this week is integrating the call to
verification inside the uploader. Development started last week and more
details on the core function can be found on the
&lt;a
class="gblog-markdown__link"
href="/diagnoseai-dev-blog/posts/week-08/#migrating-validated-documents-to-production-index"
>Week 8&lt;/a>
page. In essence, the Uploader will send an SDK trigger to the relevant Azure
Function App to cause the selected document to be moved.&lt;/p>
&lt;/li>
&lt;/ul></content></entry><entry><title>Week 8 - Chat feature and security improvements, Optimising Cognitive search indices</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-08/" rel="alternate" type="text/html" hreflang="en"/><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-08/</id><published>2024-01-23T00:00:00+00:00</published><updated>2024-01-23T00:00:00+00:00</updated><content type="html">
&lt;p>This week (Monday was 22 January 2024) was focused on three key aspects of project development. Improving security of API keys in the webapp, enabling image support for the chat API, and migrating documents from validation vector indices into production.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="api-key-security"
>
API key security
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-08/#api-key-security" class="gblog-post__anchor clip flex align-center" aria-label="Anchor API key security" href="#api-key-security">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>In the existing webapp, calls to the OpenAI endpoint were being sent directly from the frontend. This meant that they could be accessed easily from the client side. Till now this caused no issues as the app was being developed locally but it had to be improved to allow for the project to become internet deployable.&lt;/p>
&lt;p>Hence, a back-end API endpoint was created using Next.js server side runtime which stores the API keys using a .env file. This is effective at protecting the API keys and enables the web app to be deployed on the internet&lt;/p>
&lt;p>Below is an code snippet from the backend sourcecode:&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/chat-connection-api-code.png" alt="Image of the chat connection api code" />
&lt;/p>
&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="image-support-for-the-chat-api"
>
Image support for the Chat API
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-08/#image-support-for-the-chat-api" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Image support for the Chat API" href="#image-support-for-the-chat-api">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>Although an image summary and vectorisation function had already been developed, it was yet to be integrated into the websocket chat endpoint. This step is necessary to add image support into the webapp chat function.&lt;/p>
&lt;p>This involves getting an image, saving it into blob storage, converting it to base64 to be processed by the vision model, and then feeding it into the conversation history to be used by the large language model.&lt;/p>
&lt;p>Below is an image of the image functionality working in the webapp chat:
&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/web-app-chat-image.jpeg" alt="Image being uploaded to the webapp chat" />
&lt;/p>
&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="migrating-validated-documents-to-production-index"
>
Migrating validated documents to production index
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-08/#migrating-validated-documents-to-production-index" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Migrating validated documents to production index" href="#migrating-validated-documents-to-production-index">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>In the process of uploading documents, each document is assigned its own vector index in the Azure cognitive search. This allows the end-user to validate that the document has been vectorised properly and is fit to be used in the field.&lt;/p>
&lt;p>However, it is space-inefficient to store every document in its own vector index. Hence, once documents are validated they should be migrated into a production index and the validation index needs to be deleted.&lt;/p>
&lt;p>During this week, a HTTP endpoint was developed that can migrate documents from a specified index into the production index and to delete the aforementioned index after.&lt;/p>
&lt;p>Below is a code snippet from the endpoint&amp;rsquo;s source code:&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/validation-to-production-code.png" alt="Image of code snippet from validation to production endpoint" />
&lt;/p>
&lt;/p></content></entry><entry><title>Week 7 - Authentication</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-07/" rel="alternate" type="text/html" hreflang="en"/><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-07/</id><published>2024-01-19T00:00:00+00:00</published><updated>2024-01-19T00:00:00+00:00</updated><content type="html">
&lt;p>This week, the team implemented authentication with Clerk, in an
effort to be internet-ready by early February. This will be a key
milestone to deploy the project safely.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="background"
>
Background
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-07/#background" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Background" href="#background">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>This project was created for Avanade&amp;rsquo;s client companies, hence, there
are a myriad of authentication systems that they may use.&lt;/p>
&lt;p>It is hence reasoned (and confirmed with their engineers) that we can
leverage any authentication system we wanted, because it will be
modified to the individual client&amp;rsquo;s needs.&lt;/p>
&lt;p>Hence, there shouldn&amp;rsquo;t be a focus on building a nice-looking UI,
because chances are, the software will be integrated into an existing
SSO system, like Microsoft Entra, or Keycloak.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="choices"
>
Choices
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-07/#choices" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Choices" href="#choices">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>We were allowed (by our client) to choose between many authentication
systems, namely:&lt;/p>
&lt;ul>
&lt;li>Clerk (which we ended up with)&lt;/li>
&lt;li>NextAuth&lt;/li>
&lt;li>Supabase&lt;/li>
&lt;li>Auth0&lt;/li>
&lt;/ul>
&lt;p>To consider this, we had the following criteria:&lt;/p>
&lt;ol>
&lt;li>Easy to integrate with CoreAPI&lt;/li>
&lt;li>Guards all resources with relatively minimal effort&lt;/li>
&lt;li>Comes with built-in UI components&lt;/li>
&lt;/ol>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h2 id="criteria-1-easy-to-integrate-with-coreapi"
>
Criteria 1: Easy to integrate with CoreAPI
&lt;/h2>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-07/#criteria-1-easy-to-integrate-with-coreapi" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Criteria 1: Easy to integrate with CoreAPI" href="#criteria-1-easy-to-integrate-with-coreapi">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>CoreAPI, being serverless, must be able to authenticate the user as
quickly as possible, with minimal effort. When it comes to security,
we like to reduce our attack surface by relying on a more well-funded,
and battle-tested third party.&lt;/p>
&lt;p>As an additional consideration factor, CoreAPI is written in Python;
hence, the authentication method must be easily implemented in Python.&lt;/p>
&lt;p>All four options fulfilled this criteria pretty easily:&lt;/p>
&lt;ol>
&lt;li>Clerk uses JWT tokens, which can be easily verified with a public
key&lt;/li>
&lt;li>NextAuth is essentially an adapter over many authentication
services, and Clerk is one of them; hence, verification is more or
less the same as Clerk&lt;/li>
&lt;li>Supabase provides a Python SDK to obtain the user session, which
implies verification&lt;/li>
&lt;li>Auth0 has a Python SDK that seems to center around Flask, but
internally uses JWT, which hence makes verification the same as
NextAuth&lt;/li>
&lt;/ol>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h2 id="criteria-2-guards-all-resources-with-relatively-minimal-effort"
>
Criteria 2: Guards all resources with relatively minimal effort
&lt;/h2>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-07/#criteria-2-guards-all-resources-with-relatively-minimal-effort" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Criteria 2: Guards all resources with relatively minimal effort" href="#criteria-2-guards-all-resources-with-relatively-minimal-effort">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>All four authentication systems have native &lt;code>Next.js&lt;/code> components
support, which makes integrating with the two frontends relatively
simple.&lt;/p>
&lt;p>With JWT support, guarding CoreAPI endpoints can be done via a
primitive method of introducing a guard clause written as a utility
function.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h2 id="criteria-3-built-in-ui-components"
>
Criteria 3: Built-in UI components
&lt;/h2>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-07/#criteria-3-built-in-ui-components" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Criteria 3: Built-in UI components" href="#criteria-3-built-in-ui-components">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>Clerk, NextAuth and Auth0 comes with their own UI components.&lt;/p>
&lt;p>Of the three, Clerk comes built-in with the most UI components,
including:&lt;/p>
&lt;ul>
&lt;li>A default, unbranded login / signup screen (NextAuth and Auth0 has
this as well)&lt;/li>
&lt;li>A &lt;code>UserButton&lt;/code>, shown in a later screenshot&lt;/li>
&lt;/ul>
&lt;p>Based on &lt;a
class="gblog-markdown__link"
href="https://supabase.com/docs/guides/auth/auth-helpers/nextjs"
>the
Supabase&lt;/a>
docs, it appears that we would have to create the authentication UI on
our own. Furthermore, it seems that the developer would have to manage
the authentication cookies (by default, authentication data is stored
in Local Storage); while this isn&amp;rsquo;t complex and a deal-breaker, it
would be nice if library did this on their own.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h2 id="bonus-criteria-simplicity"
>
Bonus Criteria: Simplicity
&lt;/h2>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-07/#bonus-criteria-simplicity" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Bonus Criteria: Simplicity" href="#bonus-criteria-simplicity">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>This means that we now have to perform a 3-way tie-breaker between
Clerk, NextAuth and Auth0.&lt;/p>
&lt;p>NextAuth is the most flexible authentication framework, as it can use
many providers, &lt;em>including&lt;/em> Clerk. However, it does not provide the
same UI components as Clerk does (&lt;code>UserButton&lt;/code>, in particular), and
requires almost no configuration, other than setting it up as a
provider and middleware.&lt;/p>
&lt;p>Clerk is the simplest authentication framework, and is what NextAuth
calls a &lt;em>Hosted Auth&lt;/em>. This means all user information, including
avatar URLs, usernames and passwords are stored on Clerk, washing our
hands of the responsibility.&lt;/p>
&lt;p>Auth0 functions similarly to Clerk, minus the nice components we get
for free.&lt;/p>
&lt;p>Hence, we chose to use Clerk.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="result"
>
Result
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-07/#result" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Result" href="#result">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>We now have authentication for the dashboard, chatting interface, and
backend!&lt;/p>
&lt;p>The following image shows the &lt;code>UserButton&lt;/code> component on our Uploader
interface:&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/2024-01-28_11-35.png" alt="Authentication User Button" />
&lt;/p>
&lt;/p>
&lt;p>And the following shows the CoreAPI utility function to verify the JWT
token:&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/2024-01-28_11-37.png" alt="CoreAPI Authentication" />
&lt;/p>
&lt;/p>
&lt;p>Clerk is currently on allow-list mode, meaning no new sign-ups can
take place. This will allow us to deploy a demonstration safely onto
the internet.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="next-steps"
>
Next Steps
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-07/#next-steps" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Next Steps" href="#next-steps">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>An important step is also &lt;em>authorization&lt;/em>, which we have been reminded
is distinct from &lt;em>authentication&lt;/em>. For now, this is not seen as a
priority, because the software is still relatively simple.&lt;/p>
&lt;p>Our next step is to move vectors from the validation index to the
production index, so that the knowledge base has access to all
production documents during an interaction.&lt;/p></content></entry><entry><title>Week 6 - Elevator Pitch</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-06/" rel="alternate" type="text/html" hreflang="en"/><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-06/</id><published>2024-01-12T00:00:00+00:00</published><updated>2024-01-12T00:00:00+00:00</updated><content type="html">
&lt;p>When it comes to technology projects, effectively communicating your idea is as important as the technical work itself. This was the challenge we faced in preparing for the Elevator Pitch. Our goal was to create a concise, two-minute presentation that clearly conveyed our project&amp;rsquo;s objectives, outlined the problems we&amp;rsquo;re tackling, and our proposed solution.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="the-preparation"
>
The Preparation
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-06/#the-preparation" class="gblog-post__anchor clip flex align-center" aria-label="Anchor The Preparation" href="#the-preparation">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>Our preparation for the Elevator Pitch involved distilling complex concepts into clear, concise points. We focused on what mattered most, without delving too deeply into the technical aspects.&lt;/p>
&lt;p>Given the tight time limit, our content had to be carefully selected. We included:&lt;/p>
&lt;ul>
&lt;li>A clear definition of the problem and our approach to solving it&lt;/li>
&lt;li>A brief overview of the user interfaces and explanations on how they will be used&lt;/li>
&lt;li>A snapshot of our intended deliverables together with a (simplified) architecture diagram&lt;/li>
&lt;li>Potential future steps and how the clients can customise the project&lt;/li>
&lt;/ul>
&lt;p>Below is the slide deck used for the pitch:&lt;/p>
&lt;iframe src="/diagnoseai-dev-blog/elevator.pdf#navpanes=0&amp;zoom=80" style="width:820px; height:570px;" frameborder="0">&lt;/iframe>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="the-live-presentation"
>
The Live Presentation
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-06/#the-live-presentation" class="gblog-post__anchor clip flex align-center" aria-label="Anchor The Live Presentation" href="#the-live-presentation">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>The pitch was online via Zoom, with the module lead and IXN partner present. It was test of our preparation, and the rehearsal process paid off. James delivered our presentation with clarity and confidence, effectively communicating our project&amp;rsquo;s vision and potential.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="reflections-and-moving-forward"
>
Reflections and Moving Forward
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-06/#reflections-and-moving-forward" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Reflections and Moving Forward" href="#reflections-and-moving-forward">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>This experience taught us valuable lessons in communication. It highlighted the importance of being clear and direct in presenting complex technical projects. The Elevator Pitch was a brief but significant part of our project&amp;rsquo;s journey, providing us with insights that will benefit our development process moving forward.&lt;/p></content></entry><entry><title>Christmas Blog - Conversation history and validation chat</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-05/" rel="alternate" type="text/html" hreflang="en"/><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-05/</id><published>2023-12-20T00:00:00+00:00</published><updated>2023-12-20T00:00:00+00:00</updated><content type="html">
&lt;p>This Christmas break (Starting from 21st December 2023 to 9th January 2024) the team had a much lighter workload during the holidays. However, time was still spent working on these core functionalities including development of a validation chat user interface, enabling conversation history for client chat, and , creation of an Image vectorization endpoint.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="validation-chat-user-interface"
>
Validation Chat User Interface
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-05/#validation-chat-user-interface" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Validation Chat User Interface" href="#validation-chat-user-interface">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>Having developed the document upload and vectorization features the focus was now on developing the validation chat user interface. The validation chat is a part of Uploader web app and is used to validate the language model&amp;rsquo;s understanding of the manuals uploaded.&lt;/p>
&lt;p>The interface consists of validation chat message interface and two display components rendering the extracted images and text from the document uploaded. Here&amp;rsquo;s a screenshot of the validation chat:&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/validation-chat.jpeg" alt="Image of the validation chat user interface" />
&lt;/p>
&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="conversation-history"
>
Conversation History
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-05/#conversation-history" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Conversation History" href="#conversation-history">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>In an on field situation, there are multiple factors that a language model will need to keep in mind when suggesting solutions to the problems at hand. Consequently, it is important that the language model is capable of accessing the conversation history.&lt;/p>
&lt;p>After a discussion with the IXN project mentor to better understand the requirements, the team settled on using a conversation history window of ten messages as this was suited to the client requirements.&lt;/p>
&lt;p>Here&amp;rsquo;s a screenshot of the client chat accessing the conversation history.&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/client-conv-history.jpeg" alt="Image of the client chat accessing conversation history" />
&lt;/p>
&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="image-vectorization"
>
Image Vectorization
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-05/#image-vectorization" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Image Vectorization" href="#image-vectorization">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>Another important aspect of the validation chat is the ability to vectorize the extracted images. Hence, a core API feature had to be developed to accept images and upload them into a specified vector index.&lt;/p>
&lt;p>To achieve this the image was sent to a GPT4-V vision model which generated a text summary of the image. The model was prompted to create a summary that could later be used for image based content retrieval. Here&amp;rsquo;s a screenshot of the standard request contents to the vision model.&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/vision-model-prompt.png" alt="Image of the request body being sent to the language model" />
&lt;/p>
&lt;/p>
&lt;p>Further, the text generated was vectorized and its embedding was uploaded to the Azure cognitive search index specified in the request body.&lt;/p></content></entry><entry><title>Week 4 - Video</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-04/" rel="alternate" type="text/html" hreflang="en"/><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-04/</id><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><content type="html">
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="core-api"
>
Core API
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-04/#core-api" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Core API" href="#core-api">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>The proof-of-concept code mentioned in &lt;a
class="gblog-markdown__link"
href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-01/"
>the first
week&lt;/a> was used as a reference to write the chat
management component of CoreAPI.&lt;/p>
&lt;p>In particular, we implemented the following flow:&lt;/p>
&lt;ol>
&lt;li>A WebSocket proxy to the model via Azure WebPubSub Service, which
triggers a function running on Azure Function App.&lt;/li>
&lt;li>The Azure Function App connects to the Azure OpenAI model, much
like what was implemented in Proof of Concept. This means that the
model has access to Azure Cognitive Search, which is currently
acting as our vector database.&lt;/li>
&lt;li>This WebSocket proxy is connected to the front-end UI to provide an
instant-messaging like interface.&lt;/li>
&lt;/ol>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="uis"
>
UIs
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-04/#uis" class="gblog-post__anchor clip flex align-center" aria-label="Anchor UIs" href="#uis">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>While Figma generates good-looking UI, the generated code is
unserviceable. Hence, both the Uploader interfaces and Chat interfaces
were reworked to include original components.&lt;/p>
&lt;p>The instant-messaging and uploading functionality were made
functional. The following flow was implemented:&lt;/p>
&lt;ol>
&lt;li>Uploading - Files are now uploaded to a container on a Azure
Storage Account, which is then processed by a Azure Function App.&lt;/li>
&lt;li>The Azure Function App vectorizes the blob with the
&lt;code>text-embedding-ada-002&lt;/code> model, and stores the vectors within Azure
Cognitive Search.&lt;/li>
&lt;li>Chatting - When the model is invoked via CoreAPI, the model
connects to the same Azure Cognitive Search service to obtain
relevant documentation whenever required.&lt;/li>
&lt;/ol>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="video"
>
Video
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-04/#video" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Video" href="#video">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>A video was created to demo the progress made thus far:&lt;/p>
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/4eF8jrSBjlU?si=Y9TLpTn2iyxHKrkX" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>&lt;/iframe>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="christmas"
>
Christmas
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-04/#christmas" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Christmas" href="#christmas">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>Over the Christmas break, we will focus on the following components:&lt;/p>
&lt;ol>
&lt;li>Validation Chat User Interface&lt;/li>
&lt;li>User Conversation History (at the moment, it is not stored)&lt;/li>
&lt;li>Image Vectorization&lt;/li>
&lt;/ol></content></entry><entry><title>Week 3 - UI's and Backend Development</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-03/" rel="alternate" type="text/html" hreflang="en"/><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-03/</id><published>2023-12-05T00:00:00+00:00</published><updated>2023-12-05T00:00:00+00:00</updated><content type="html">
&lt;p>This week marks the start of development, having established the requirements with the client. We focused mainly on building the individual components - UI&amp;rsquo;s and document storage and processing, but also made a start on integration.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="user-interfaces"
>
User Interfaces
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-03/#user-interfaces" class="gblog-post__anchor clip flex align-center" aria-label="Anchor User Interfaces" href="#user-interfaces">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>There was a big emphasis on user interface implementation for week 3. The chat webapp interface was originally designed using React. We then decided that this was unsuitable because of the nature of the project (communication with the chat backend). Hence the webapp was transitioned to Next.js and now offers server-side rendering, and more importanty - secure API calls without exposing keys.&lt;/p>
&lt;p>Below is the current version of our uploader interface:&lt;/p>
&lt;img src="chat_webapp.png" alt="Image of the chat webapp" width=440>
&lt;p>The first design of the uploader was also developed this week from the Figma mock.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="blob-storage-and-cognitive-search"
>
Blob Storage and Cognitive Search
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-03/#blob-storage-and-cognitive-search" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Blob Storage and Cognitive Search" href="#blob-storage-and-cognitive-search">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>We decided that 2 Blob Storage containers will be used. The first one called &amp;ldquo;verification&amp;rdquo; will only have 1 document in it at a time. The document comes from the uploader and all the text will be extracted for verification purposes. The user can view and inquire about this text, and if the document is approved it will be moved to the &amp;ldquo;production&amp;rdquo; containter, where all the approved documents are stored.&lt;/p>
&lt;p>In order to access our knowledge base we deployed a Cognitive Search model to index of all the aproved documents and query (through vector search). The embeddings model ada-002 is used to generate all the vector representations of the documents so that they can be added to the index.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="integration"
>
Integration
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-03/#integration" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Integration" href="#integration">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>In terms of integration, this week we prioritised two key areas:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Chat Integration: connecting the Next.js frontend to our backend through WebSockets. This approach was favoured because of the reliablity and low latency of WebSockets&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Uploader and Blob Storage: a crucial connection, enabling the smooth transfer of data from our uploader to blob storage. This integration is vital for data processing, forming a core part of our system&amp;rsquo;s architecture.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="ci"
>
CI
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-03/#ci" class="gblog-post__anchor clip flex align-center" aria-label="Anchor CI" href="#ci">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>In addition to all the frontend and backend development we have set up CI through GitHub Actions on the dev-blog repository.
The workflow activates when changes are pushed to main or a pull request is made. Its purpose is to verify that the blog can be built and deployed correctly.&lt;/p></content></entry><entry><title>Week 2 - Gathering requirements</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-02/" rel="alternate" type="text/html" hreflang="en"/><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-02/</id><published>2023-11-20T00:00:00+00:00</published><updated>2023-11-20T00:00:00+00:00</updated><content type="html">
&lt;p>This week (Monday was 20 November 2023) first week after the reading week, was primarily focused on gathering and formalizing client requirements and presenting a finalised requirements document to the client. Additionally, time was spent into breaking down the tasks required to develop the Core, Uploader and Web systems.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="requirements-document"
>
Requirements document
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-02/#requirements-document" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Requirements document" href="#requirements-document">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>Having estimated the time and effort it would require to implement the different features proposed to the client we came up with a formal requirement documents.&lt;/p>
&lt;p>Definitions were established for different features and stakeholders in the project. To avoid any ambiguity in the intepretation of the requirements document.&lt;/p>
&lt;p>Further, the requirements were broken down into functional, non-functional, system and user requirements. In addition, care was taken to distinguish the requirements that the system has to have and those that are optional using the keywords shall and should.&lt;/p>
&lt;p>The aforementioned requirements document can be found via &lt;a
class="gblog-markdown__link"
href="/diagnoseai-dev-blog/Requirements.pdf"
>this link&lt;/a>&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="task-breakdown"
>
Task breakdown
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-02/#task-breakdown" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Task breakdown" href="#task-breakdown">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>Tasks for developing the three main aspects of the project i.e. Uploader, Web and , Core were identified. These tasks were added to the Kanban project board with the appropriate links.&lt;/p>
&lt;p>The Kanban board can be found &lt;a
class="gblog-markdown__link"
href="https://github.com/orgs/2023-comp0016-avanade-team4/projects/2"
>here&lt;/a>. Here is a screenshot of the todo section: &lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/kanban-todo.png" alt="Image of todo section of the kanban" />
&lt;/p>
&lt;/p></content></entry><entry><title>Week 1 - Architecture Design</title><link href="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-01/" rel="alternate" type="text/html" hreflang="en"/><id>https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-01/</id><published>2023-11-13T23:38:32+00:00</published><updated>2023-11-13T23:38:32+00:00</updated><content type="html">
&lt;p>This week (Monday was 13 November 2023) was reading week, which
consisted of the deadline for our Human Centered Interface (HCI)
design slides.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="hci-deliverable"
>
HCI Deliverable
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-01/#hci-deliverable" class="gblog-post__anchor clip flex align-center" aria-label="Anchor HCI Deliverable" href="#hci-deliverable">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>After iterating our sketches and prototype with our client, they were
fairly happy with the final interactive prototype, which we
incorporated into our slides.&lt;/p>
&lt;p>The aforementioned slides can be found via &lt;a
class="gblog-markdown__link"
href="/diagnoseai-dev-blog/Report.pdf"
>this link&lt;/a>. Here is
a small screenshot of it:&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/2023-11-27_23-51.png" alt="image of first slide of diagnoseai" />
&lt;/p>
&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="proof-of-concept-code"
>
Proof of Concept code
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-01/#proof-of-concept-code" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Proof of Concept code" href="#proof-of-concept-code">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>To demonstrate the feasibility of this project, we whipped up a script
that does the following things:&lt;/p>
&lt;ol>
&lt;li>Gets triggered by an upload to a Blob storage, via Azure Function Apps.&lt;/li>
&lt;li>Parses a document, and extracts the text (using &lt;a
class="gblog-markdown__link"
href="https://azure.microsoft.com/en-gb/products/ai-services/ai-document-intelligence"
>Document Intelligence&lt;/a>);&lt;/li>
&lt;li>Sends the text to a text embedding model, such as
&lt;code>text-embedding-ada-002&lt;/code> to get embeddings;&lt;/li>
&lt;li>Store the embeddings into a vector store, such as &lt;a
class="gblog-markdown__link"
href="https://azure.microsoft.com/en-gb/products/ai-services/ai-search"
>Azure AI Search&lt;/a>.&lt;/li>
&lt;/ol>
&lt;p>Then, we configured Azure AI Search as a data source to an Azure
OpenAI deployment, which, for the purposes of demonstration, was
&lt;code>gpt-35-turbo&lt;/code>.&lt;/p>
&lt;p>The results were as expected; the model was able to support its
answers with phrases quoted from the text, and even provided
references based on the uploaded documents:&lt;/p>
&lt;p>&lt;p class="md__image">
&lt;img src="/diagnoseai-dev-blog/images/demo-poc-1.jpg" alt="image of demo response" />
&lt;/p>
&lt;/p>
&lt;p>The code for this is available in &lt;a
class="gblog-markdown__link"
href="https://github.com/2023-comp0016-avanade-team4/diagnoseai-core/blob/7910e761ddbae4e76b7115ed9ce519730f93787a/function_app.py"
>commit 7910e76 (requires
authorization)&lt;/a>.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="github"
>
GitHub
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-01/#github" class="gblog-post__anchor clip flex align-center" aria-label="Anchor GitHub" href="#github">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>We set up a GitHub organisation account containing our code (and this
blog), and shared it with our IXN mentor.&lt;/p>
&lt;p>The GitHub organisation can be found via &lt;a
class="gblog-markdown__link"
href="https://github.com/orgs/2023-comp0016-avanade-team4"
>this
link&lt;/a>.&lt;/p>
&lt;p>We also share a Kanban board, which can be found
&lt;a
class="gblog-markdown__link"
href="https://github.com/orgs/2023-comp0016-avanade-team4/projects/2"
>here&lt;/a>.&lt;/p>
&lt;div class="flex align-center gblog-post__anchorwrap">
&lt;h1 id="next-week"
>
Next week
&lt;/h1>
&lt;a data-clipboard-text="https://2023-comp0016-avanade-team4.github.io/diagnoseai-dev-blog/posts/week-01/#next-week" class="gblog-post__anchor clip flex align-center" aria-label="Anchor Next week" href="#next-week">
&lt;svg class="gblog-icon gblog_link">&lt;use xlink:href="#gblog_link">&lt;/use>&lt;/svg>
&lt;/a>
&lt;/div>
&lt;p>We decided to start drawing up some requirements, and create an
overall systems architecture diagram to better communicate what we
want to achieve by the end of the project.&lt;/p></content></entry></feed>